{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00.Introduction_to_Apache_Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yzU_4EjAjZgh"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S3gam/EDEM-Data-Analytics/blob/main/00_Introduction_to_Apache_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9d0x1OTh2N"
      },
      "source": [
        "# Prerrequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_DQBVj_KNvL"
      },
      "source": [
        "Installing Spark\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEbGSM3_NM-z"
      },
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!pip -q install findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooP8hZlothY4"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqaK14EOtgxj"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdFv-xxITa2J"
      },
      "source": [
        "Starting Spark Session and print the version\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDLMbVBATf9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "54918a92-0bfe-48d0-bdfb-d801153f14b0"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# create the session - Esto es lo primero que se hace siempre con Spark\n",
        "\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.2.0'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNiYuI5dGo8Y"
      },
      "source": [
        "Creating tunnel</br>\n",
        "**To Check the Spark UI, open the URL printed by running the above command : https://######/jobs/, /SQL/**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4-7fXZiGmqB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b8d860c-85b4-4f51-90f0-c889955c4cce"
      },
      "source": [
        " from google.colab.output import eval_js\n",
        " print(eval_js(\"google.colab.kernel.proxyPort(4040)\") + \"jobs/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://hiiuly8awhq-496ff2e9c6d22116-4040-colab.googleusercontent.com/jobs/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z0h3dF9Vg4X"
      },
      "source": [
        "# Descargar Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBDin-0sXgyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6acb437b-cc19-48cf-c29f-fd485691c58f"
      },
      "source": [
        "# We download some datasets we need for exercices\n",
        "\n",
        "!mkdir -p /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/frankenstein.txt -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/el_quijote.txt -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/characters.csv -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/planets.csv -P /dataset\n",
        "!ls /dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "characters.csv\tel_quijote.txt\tfrankenstein.txt  planets.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02Zwm3NRXS_I"
      },
      "source": [
        "# RDD\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o6f6QOjTcZ"
      },
      "source": [
        "## Example 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnbafeFCVk8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9758e4d3-a88b-49c4-a66b-097b06634f0b"
      },
      "source": [
        "textFile = spark.sparkContext.textFile(\"/dataset/frankenstein.txt\") # Este comando está leyendo el fichero y guardándolo en una variable\n",
        "textFile.first() # Esta función devuelve la primera linea del fichero"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'FRANKENSTEIN'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a00GmwOZmM2"
      },
      "source": [
        "\n",
        "Creation of paralelized collection de colecciones paralelizadas\n",
        "This is a fast way to create a RDD:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzU_4EjAjZgh"
      },
      "source": [
        "## Example 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función **parallelize** Convierte una estructura de código en RDD (lo parte y lo comparte por los nodos)\n"
      ],
      "metadata": {
        "id": "PKDWuRlB8R1I"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgireGq6YWEj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d69e42-4d4b-495e-c27a-59e85ce85753"
      },
      "source": [
        "distData = spark.sparkContext.parallelize([25, 20, 15, 10, 5]) \n",
        "distData.reduce(lambda x ,y: x + y) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX0FXU7JawRm"
      },
      "source": [
        "## Exercise 1\n",
        "Count the number of lines for `el_quijote.txt` file\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCDo_-PiaEVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fcc1b9d-4d52-4c5c-96a4-f99e067cef2c"
      },
      "source": [
        "textFile2 = spark.sparkContext.textFile(\"/dataset/el_quijote.txt\") # Este comando está leyendo el fichero y guardándolo en una variable\n",
        "textFile2.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2186"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prvVhMD4a5o7"
      },
      "source": [
        "## Exercise 2\n",
        "Print the first line of the file `el_quijote.txt`\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vgL2Upsa-Qg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "74c09d98-acac-4257-e2bd-1b9e0def7dc3"
      },
      "source": [
        "textFile2.first()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'DON QUIJOTE DE LA MANCHA'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAxxBSrBb92y"
      },
      "source": [
        "## Transformations and Actions in RDDs "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-jBGb_acVuZ"
      },
      "source": [
        "### Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fc-rQBNjnNi"
      },
      "source": [
        "### Example 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvxep4yubxtC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "814cf73a-9108-4ef2-cabb-d39619127abf"
      },
      "source": [
        "print(textFile2.count()) # Number of elements in RDD\n",
        "print(textFile2.first()) # First element in RDD"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2186\n",
            "DON QUIJOTE DE LA MANCHA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYhM504ycl9K"
      },
      "source": [
        "### Transformaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irxzzmfwjyYi"
      },
      "source": [
        "### Example 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpkUk7t9cfoL"
      },
      "source": [
        "# ReduceByKey\n",
        "lines = spark.sparkContext.textFile(\"/dataset/frankenstein.txt\") # Leemos frankestein.txt\n",
        "pairs = lines.map(lambda s: (s, 1)) # Generamos una clave y un valor, cada linea es un 1. Estamos contando filas\n",
        "counts = pairs.reduceByKey(lambda a, b: a + b).cache()  \n",
        "counts.count() \n",
        "counts.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfajHIRsejog"
      },
      "source": [
        "# SortByKey\n",
        "sorted = counts.sortByKey()\n",
        "sorted.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6c2qVSLj4Cy"
      },
      "source": [
        "### Example 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDwoLMbbdGPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71d089f-430f-44cf-c1ea-c60494be7a6f"
      },
      "source": [
        "# Filter\n",
        "\n",
        "linesWithSpark = textFile.filter(lambda line: \"the\" in line) # Filtramos el texto entero y encontrar la palabra \"the\"\n",
        "linesWithSpark.count() # Con esto contamos el número de veces que sale la palabra \"the\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3712"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngS6b5jUfYen"
      },
      "source": [
        "### Exercise 3\n",
        "Get the word count for the file `frankenstein.txt`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqbwlZn8fVNd"
      },
      "source": [
        "# ReduceByKey EJEMPLO DE ALVARO ( Contamos las palabras que tiene )\n",
        "\n",
        "lines = spark.sparkContext.textFile(\"/dataset/frankenstein.txt\")\n",
        "contarPalabras = lines.flatMap(lambda linea: linea.split(\" \")).countByValue()\n",
        "\n",
        "for palabra, contador in contarPalabras.items():\n",
        "    print(\"{} : {}\".format(palabra, contador))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReduceByKey EJEMPLO DE LUIS ( Contamos las palabras que tiene )\n",
        "\n",
        "lines = spark.sparkContext.textFile(\"/dataset/frankenstein.txt\")\n",
        "pairs = lines.flatMap(lambda a: a.split(\" \")).map(lambda a: (a, 1))\n",
        "counts = pairs.reduceByKey(lambda a, b: a + b).cache()\n",
        "counts.collect()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3UZkl6zhLsXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReduceByKey EJEMPLO DEL PROFE ( Contamos las palabras que tiene )\n",
        "\n",
        "\n",
        "lines = spark.sparkContext.textFile(\"/dataset/frankenstein.txt\")\n",
        "counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
        "             .map(lambda word: (word, 1)) \\\n",
        "             .reduceByKey(lambda a, b: a + b)\n",
        "counts.collect()\n"
      ],
      "metadata": {
        "id": "723eVzbXPGPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "034ZWkexhXQF"
      },
      "source": [
        "# Exercise 4\n",
        "Get TOP 10 of the words with more than 4 characters\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EJEMPLO DEL PROFE ( contamos los caracteres por palabra, filtramos por mas de 4 palabras y mostramos las 10 que más salen )\n",
        "\n",
        "lines = spark.sparkContext.textFile(\"/dataset/frankenstein.txt\")\n",
        "\n",
        "lines.flatMap(lambda line: line.split(\" \")) \\\n",
        "    .filter (lambda word: len(word)>4)\\\n",
        "    .map(lambda word: (word, 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b)\\\n",
        "    .map(lambda word: (word[1], word[0]))\\\n",
        "    .sortByKey(False)\\\n",
        "    .take(10)"
      ],
      "metadata": {
        "id": "wpU1BFBHN0Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QjYLZ0MgJ1_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IqqLY6PRtGg"
      },
      "source": [
        "## Key/Value Pair RDD\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq6NhwCXRl7n"
      },
      "source": [
        "### Example 6\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R04SZu82R1ui"
      },
      "source": [
        "charac_sw = spark.sparkContext.textFile(\"/dataset/characters.csv\")\n",
        "planets_sw = spark.sparkContext.textFile(\"/dataset/planets.csv\")\n",
        "charac_sw.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fuMXUgnS_Rb"
      },
      "source": [
        "planets_sw.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbYKGuvxPiqb"
      },
      "source": [
        "from itertools import islice\n",
        "\n",
        "charac_sw_noheader = charac_sw.mapPartitionsWithIndex(\n",
        "    lambda idx, it: islice(it, 1, None) if idx == 0 else it)\n",
        "\n",
        "planets_sw_noheader = planets_sw.mapPartitionsWithIndex(\n",
        "    lambda idx, it: islice(it, 1, None) if idx == 0 else it)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1BEEwqpTREy"
      },
      "source": [
        "### Exercise 5\n",
        "Get a list of the population of the planet each Star Wars character belongs to\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo0dq-QrU0MU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}