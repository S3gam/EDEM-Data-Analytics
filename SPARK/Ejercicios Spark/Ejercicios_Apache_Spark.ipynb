{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicios_Apache_Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9d0x1OTh2N"
      },
      "source": [
        "# Prerrequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_DQBVj_KNvL"
      },
      "source": [
        "Installing Spark\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEbGSM3_NM-z"
      },
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!pip -q install findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP_HtvSAj4sI"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhjOxtJWb5Y1"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G28MgeRJHKJ5"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# create the session\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNiYuI5dGo8Y"
      },
      "source": [
        "Creating tunnel</br>\n",
        "**To Check the Spark UI, open the URL printed by running the above command : https://######/jobs/, /SQL/**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4-7fXZiGmqB"
      },
      "source": [
        " from google.colab.output import eval_js\n",
        " print(eval_js(\"google.colab.kernel.proxyPort(4040)\") + \"jobs/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z0h3dF9Vg4X"
      },
      "source": [
        "# Descargar Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBDin-0sXgyI"
      },
      "source": [
        "!mkdir -p /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/food_inspections_lite.csv -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/risk_description.csv -P /dataset\n",
        "!ls /dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prvVhMD4a5o7"
      },
      "source": [
        "# Ejercicio 1\n",
        "**Crea un DataFrame como resultado de la lectura del fichero \"/dataset/food_inspections_lite.csv\"**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vgL2Upsa-Qg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-HOEezxVnCe"
      },
      "source": [
        "# Ejercicio 2\n",
        "**Obtén el número de inspecciones distintas con Riesgo alto \"Risk 1 (High)\"**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ88z1pQV9WF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R1kpKmIXi-h"
      },
      "source": [
        "# Ejercicio 3\n",
        "**Crea un DataFrame como resultado de la lectura del fichero \"/dataset/risk_description.csv\"**<br>\n",
        "**Obtén una tabla con las siguientes columnas: <br>**\n",
        "1. DBA Name\n",
        "2. Facility Type\n",
        "3. Risk\n",
        "4. Risk description\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-8W6zmhacJi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX1ahdYyb2L9"
      },
      "source": [
        "# Ejercicio 4\n",
        "**Accede a la Spark UI para ver el plan de ejecución. Describe cada una de las piezas/cajas que componen el plan de ejecución (Una descripción breve de una línea por caja será suficiente).**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sQNdQ2Gbz4p"
      },
      "source": [
        "# Ejercicio 5\n",
        "**1. Para cada local (DBA Name) y su resultado (Results), obtén el número de inspecciones que ha tenido**<br><br>\n",
        "**2. Obtén las dos locales (DBA Name) que más inspecciones han tenido por cada uno de los resultados**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUb4H61IeYfM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}