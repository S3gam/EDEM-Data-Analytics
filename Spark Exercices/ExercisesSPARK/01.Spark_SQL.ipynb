{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01.Spark_SQL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Jq9d0x1OTh2N",
        "8Z0h3dF9Vg4X"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9d0x1OTh2N"
      },
      "source": [
        "# Prerrequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_DQBVj_KNvL"
      },
      "source": [
        "Installing Spark\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEbGSM3_NM-z"
      },
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!pip -q install findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooP8hZlothY4"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LqjbMdHw4CV"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdFv-xxITa2J"
      },
      "source": [
        "Starting Spark Session and print the version\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDLMbVBATf9K"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# create the session\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNiYuI5dGo8Y"
      },
      "source": [
        "Creating tunnel</br>\n",
        "**To Check the Spark UI, open the URL printed by running the above command : https://######/jobs/, /SQL/**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4-7fXZiGmqB"
      },
      "source": [
        " from google.colab.output import eval_js\n",
        " print(eval_js(\"google.colab.kernel.proxyPort(4040)\") + \"jobs/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z0h3dF9Vg4X"
      },
      "source": [
        "# Download Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBDin-0sXgyI"
      },
      "source": [
        "!mkdir -p /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/bank.csv -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/vehicles.csv -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/characters.csv -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/netflix_titles.csv -P /dataset\n",
        "!ls /dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02Zwm3NRXS_I"
      },
      "source": [
        "# Reading Data with Spark SQL\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o6f6QOjTcZ"
      },
      "source": [
        "## Example 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USrsDETtstv-"
      },
      "source": [
        "!head /dataset/bank.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjPpxRyJYA1h"
      },
      "source": [
        "Converting a RDD to a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnbafeFCVk8d"
      },
      "source": [
        "from pyspark.sql.types import Row\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "bankText = spark.sparkContext.textFile(\"/dataset/bank.csv\")\n",
        "\n",
        "bank = bankText.map(lambda lineaCsv: lineaCsv.split(\";\"))\\\n",
        ".filter(lambda s: s[0] != \"\\\"age\\\"\") \\\n",
        ".map(lambda row: Row(int(row[0]), row[1].replace(\"\\\"\", \"\"), row[2].replace(\"\\\"\", \"\"), row[3].replace(\"\\\"\", \"\"), row[5].replace(\"\\\"\", \"\"))) \\\n",
        ".toDF([\"age\", \"job\", \"marital\", \"education\", \"balance\"]) \\\n",
        ".withColumn(\"age\", col(\"age\").cast(\"int\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BEWmhhYqxp8"
      },
      "source": [
        "bank.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7kQm1A5q00M"
      },
      "source": [
        "bank.createOrReplaceTempView(\"bank\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-JI8uCsroG7"
      },
      "source": [
        "Loading a **Google Colab extension** to show a table with filters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCGzK_zfrm8u"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE6JMclKbbkw"
      },
      "source": [
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "bank_grouped = bank\\\n",
        ".groupBy(bank.marital) \\\n",
        ".agg({\"balance\": \"avg\"}) \\\n",
        ".select(\"marital\", col(\"avg(balance)\").alias(\"balance_avg\")) \\\n",
        ".orderBy(col(\"balance_avg\").desc())\\\n",
        "\n",
        "bank_grouped.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ1vduJ45VtS"
      },
      "source": [
        "bank_grouped.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTy8ySNSpgpN"
      },
      "source": [
        "spark.sql(\"SELECT marital, avg(balance) as balance_avg FROM bank group by marital\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CJgXJe-kpnF"
      },
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.pie(bank_grouped.toPandas(), values='balance_avg', names='marital', title='By Marital')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzU_4EjAjZgh"
      },
      "source": [
        "## Example 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV-NchsMsHvF"
      },
      "source": [
        "Loading a CSV file as RDD, converting into a DataFrame, applying a specific schema using the method `createDataFrame`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgireGq6YWEj"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "bankSchema = StructType([\n",
        "    StructField(\"age\", IntegerType(), False), \n",
        "    StructField(\"job\", StringType(), False),\n",
        "    StructField(\"marital\", StringType(), False),\n",
        "    StructField(\"education\", StringType(), False),\n",
        "    StructField(\"balance\", IntegerType(), False)])\n",
        "\n",
        "bankText = spark.sparkContext.textFile(\"/dataset/bank.csv\")\n",
        "\n",
        "bank = bankText\\\n",
        ".map(lambda s: s.split(\";\")).filter(lambda s: s[0] != \"\\\"age\\\"\")\\\n",
        ".map(lambda s:(int(s[0]), str(s[1]).replace(\"\\\"\", \"\"), str(s[2]).replace(\"\\\"\", \"\"), str(s[3]).replace(\"\\\"\", \"\"), int(s[5]) ))\n",
        "\n",
        "bankdf = spark.createDataFrame(bank, bankSchema)\n",
        "bankdf.createOrReplaceTempView(\"bank2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTSxYSiqtwum"
      },
      "source": [
        "spark.sql(\"select * from bank2 limit 10\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX0FXU7JawRm"
      },
      "source": [
        "## Exercise 1\n",
        "Load file `vehicles.csv` to a DataFrame, showing the content and printing the schema.\n",
        "\n",
        "Use this [documentation](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html) to read data in a DataFrame\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWdg2O5bwhV-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCDfqm_UxUcn"
      },
      "source": [
        "Filter out the previous DafaFrame to get vehicles where the capicity is greater than 70\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IdhtILfwmSF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GUV9KLzqR-4"
      },
      "source": [
        "# Spark SQL. Aggregation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbMEyQFOqVnQ"
      },
      "source": [
        "Useful Links:\n",
        "\n",
        "http://spark.apache.org/docs/latest/api/python/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZw41ki6kc3p"
      },
      "source": [
        "## Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAzx5B5IkVeP"
      },
      "source": [
        "Using the DataFrame with all vehicles loaded in Exercise 1, get the number of passengers by vehicle class\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIKCDk4mx0ep"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSEBVnVQlU52"
      },
      "source": [
        "## Exercise 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF4fyAaMlXsI"
      },
      "source": [
        "Load the file `characters.csv` getting the most common eye color among all characters\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ_HrY7uk3bM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT-AFIMvm43z"
      },
      "source": [
        "## Exercise 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9UBVU6ym9ue"
      },
      "source": [
        "1. Load characters DataFrame into a temporary table\n",
        "2. Using SQL, get the number of characters by gender\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdBgPJ8bnvsq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv0AmHi926F7"
      },
      "source": [
        "## Exercise 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR6FPU3b29TP"
      },
      "source": [
        "Load `netflix_titles.csv` file in a DataFrame, printing the schema\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zyuYbll3UrI"
      },
      "source": [
        "!head /dataset/netflix_titles.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3qnxQT_3cd3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG81SgpY4P3t"
      },
      "source": [
        "## Exercise 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56fGSsmt4adO"
      },
      "source": [
        "Get the year in which most films were added(No TV Shows). Use a UDF to get the year\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5O7kVYDBStC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}