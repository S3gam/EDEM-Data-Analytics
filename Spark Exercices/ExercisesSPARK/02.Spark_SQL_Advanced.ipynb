{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02.Spark_SQL_Advanced.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Jq9d0x1OTh2N",
        "8Z0h3dF9Vg4X",
        "h1o6f6QOjTcZ",
        "tv0AmHi926F7"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9d0x1OTh2N"
      },
      "source": [
        "# Prerrequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_DQBVj_KNvL"
      },
      "source": [
        "Installing Spark\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEbGSM3_NM-z"
      },
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!pip -q install findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP_HtvSAj4sI"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhjOxtJWb5Y1"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G28MgeRJHKJ5"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# create the session\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNiYuI5dGo8Y"
      },
      "source": [
        "Creating tunnel</br>\n",
        "**To Check the Spark UI, open the URL printed by running the above command : https://######/jobs/, /SQL/**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4-7fXZiGmqB"
      },
      "source": [
        " from google.colab.output import eval_js\n",
        " print(eval_js(\"google.colab.kernel.proxyPort(4040)\") + \"jobs/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z0h3dF9Vg4X"
      },
      "source": [
        "# Download Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBDin-0sXgyI"
      },
      "source": [
        "!mkdir -p /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/bank.csv -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/vehicles.csv -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/characters.csv -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/planets.csv -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/species.csv -P /dataset\n",
        "!ls /dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02Zwm3NRXS_I"
      },
      "source": [
        "# Windows Partitioning\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o6f6QOjTcZ"
      },
      "source": [
        "## Example 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USrsDETtstv-"
      },
      "source": [
        "!head /dataset/bank.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjPpxRyJYA1h"
      },
      "source": [
        "Reading data from `bank.csv` file to a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnbafeFCVk8d"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "bank_df = spark.read.format(\"csv\") \\\n",
        "  .option(\"sep\", \";\") \\\n",
        "  .option(\"inferSchema\", \"true\") \\\n",
        "  .option(\"header\", \"true\") \\\n",
        "  .load(\"/dataset/bank.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxhLiwwXk6kD"
      },
      "source": [
        "bank_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHDroSQ1ZQSB"
      },
      "source": [
        "Get the balance of the two youngest people by job\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BEWmhhYqxp8"
      },
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "byJob = Window.partitionBy(\"job\").orderBy(\"age\")\n",
        "\n",
        "bank_df \\\n",
        "  .withColumn(\"new_column_job\", row_number().over(byJob)) \\\n",
        "  .filter(col(\"new_column_job\") <= 2) \\\n",
        "  .select(\"age\", \"job\", \"balance\") \\\n",
        "  .orderBy(\"job\", \"age\") \\\n",
        "  .show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzU_4EjAjZgh"
      },
      "source": [
        "## Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV-NchsMsHvF"
      },
      "source": [
        "Using the dataframe built from `bank.csv`file, get the TOP 3 of maximum balance by marital\n",
        "Obtén el Top 3 de máximos balances por estado civil\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgireGq6YWEj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX0FXU7JawRm"
      },
      "source": [
        "## Exercise 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBtPRPnVmpGM"
      },
      "source": [
        "Load `vehicles.csv` file into a DataFrame\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYPt59chwJUw"
      },
      "source": [
        "!head /dataset/vehicles.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCDo_-PiaEVl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCDfqm_UxUcn"
      },
      "source": [
        "For each vehicle, get the difference in price (`cost_in_credits`) for each product compared to the cheapest product in the same vehicle class\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n2R6MJlxP4w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GUV9KLzqR-4"
      },
      "source": [
        "# Joins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZw41ki6kc3p"
      },
      "source": [
        "## Exercise 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAzx5B5IkVeP"
      },
      "source": [
        "1. Create dataframes for files `characters.csv` and `planets.csv`\n",
        "2. Get the planet gravity for each character, selecting only the character name, planet name and gravity.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMlJTJzTkC2j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSEBVnVQlU52"
      },
      "source": [
        "## Exercise 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF4fyAaMlXsI"
      },
      "source": [
        "Check exercise 3. What join type are been used? Why?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbYUbaSMlXGc"
      },
      "source": [
        "After checking execution plan, execute the following instructions:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8HYt2Eylh4k"
      },
      "source": [
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", '0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwEnVoVqnj7D"
      },
      "source": [
        "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxqgghUnlWe3"
      },
      "source": [
        "Execute again the query of the exercise 3\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT-AFIMvm43z"
      },
      "source": [
        "## Exercise 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9UBVU6ym9ue"
      },
      "source": [
        "1. Create a DataFrame from `species.csv`.\n",
        "2. Repartition the previous DataFrame to 100 partitions\n",
        "3. Repartition `characters` DataFrame to 100 partitions\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohStdyUGnRC0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv0AmHi926F7"
      },
      "source": [
        "## Exercise 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR6FPU3b29TP"
      },
      "source": [
        "Get the specie classification for each character. Select only the character name and its classification<br>\n",
        "Use DataFrames repartitioned previously\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zyuYbll3UrI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG81SgpY4P3t"
      },
      "source": [
        "## Exercise 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56fGSsmt4adO"
      },
      "source": [
        "1. Execute the following statement over the DataFrame built in exercise 6\n",
        "2. Check the difference in terms of rows distribution across all partitions\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFvUIbn93woD"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "classDF \\\n",
        "  .withColumn(\"partitionId\", spark_partition_id()) \\\n",
        "  .groupBy(\"partitionId\") \\\n",
        "  .count() \\\n",
        "  .orderBy(col(\"count\").desc()) \\\n",
        "  .show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io-BtgnHyKZ6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}