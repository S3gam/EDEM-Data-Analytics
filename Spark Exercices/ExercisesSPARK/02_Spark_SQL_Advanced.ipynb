{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02.Spark_SQL_Advanced_Solution.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Jq9d0x1OTh2N",
        "8Z0h3dF9Vg4X",
        "h1o6f6QOjTcZ",
        "tv0AmHi926F7"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9d0x1OTh2N"
      },
      "source": [
        "# Prerrequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_DQBVj_KNvL"
      },
      "source": [
        "Installing Spark and Apache Kafka Library in VM\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEbGSM3_NM-z"
      },
      "source": [
        "# install Java8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# download spark3.0.1\n",
        "!wget -q https://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip it\n",
        "!tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install py4j\n",
        "\n",
        "# For maps\n",
        "!pip install folium\n",
        "!pip install plotly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0qDLAxMTUYQ"
      },
      "source": [
        "Define the environment (Java & Spark homes)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSd4dfANNndH"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[*] pyspark-shell\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_IgZEv2XaDm"
      },
      "source": [
        "Starting Spark Session and print the version\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDLMbVBATf9K"
      },
      "source": [
        "import findspark\n",
        "findspark.init(\"spark-3.0.1-bin-hadoop3.2\")# SPARK_HOME\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create the session\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.ui.port\", \"4500\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G28MgeRJHKJ5"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPrOO29YRuDB"
      },
      "source": [
        "# For Pandas conversion optimization\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNiYuI5dGo8Y"
      },
      "source": [
        "Creating ngrok tunnel to allow Spark UI (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4-7fXZiGmqB"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip\n",
        "!sleep 2\n",
        "get_ipython().system_raw('./ngrok http 4500 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z0h3dF9Vg4X"
      },
      "source": [
        "# Descargar Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBDin-0sXgyI"
      },
      "source": [
        "!mkdir -p /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/bank.csv -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/vehicles.csv -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/characters.csv -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/planets.csv -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/species.csv -P /dataset\n",
        "!ls /dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02Zwm3NRXS_I"
      },
      "source": [
        "# Windows Partitioning\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o6f6QOjTcZ"
      },
      "source": [
        "## Ejemplo 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USrsDETtstv-"
      },
      "source": [
        "!head /dataset/bank.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjPpxRyJYA1h"
      },
      "source": [
        "Leyendo Datos del fichero bank.csv a un Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnbafeFCVk8d"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "bank_df = spark.read.format(\"csv\") \\\n",
        "  .option(\"sep\", \";\") \\\n",
        "  .option(\"inferSchema\", \"true\") \\\n",
        "  .option(\"header\", \"true\") \\\n",
        "  .load(\"/dataset/bank.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxhLiwwXk6kD"
      },
      "source": [
        "bank_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHDroSQ1ZQSB"
      },
      "source": [
        "**Obtén el balance de las dos personas más jóvenes por tipo de trabajo**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BEWmhhYqxp8"
      },
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "byJob = Window.partitionBy(\"job\").orderBy(\"age\")\n",
        "\n",
        "bank_df \\\n",
        "  .withColumn(\"new_column_job\", row_number().over(byJob)) \\\n",
        "  .filter(col(\"new_column_job\") <= 2) \\\n",
        "  .select(\"age\", \"job\", \"balance\") \\\n",
        "  .orderBy(\"job\", \"age\") \\\n",
        "  .show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzU_4EjAjZgh"
      },
      "source": [
        "## Ejercicio 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV-NchsMsHvF"
      },
      "source": [
        "**A partir del Dataframe formado a partir del fichero \"bank.csv\". \n",
        "Obtén el Top 3 de máximos balances por estado civil**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgireGq6YWEj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX0FXU7JawRm"
      },
      "source": [
        "## Ejercicio 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBtPRPnVmpGM"
      },
      "source": [
        "**Carga el fichero de vehicles.csv en un DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYPt59chwJUw"
      },
      "source": [
        "!head /dataset/vehicles.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6-LoYNzfZFp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCDfqm_UxUcn"
      },
      "source": [
        "**Para cada uno de los vehículos, obtén la diferencia de precio (*cost_in_credits*) para cada producto con respecto al más barato en la misma clase de vehículo**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IdhtILfwmSF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GUV9KLzqR-4"
      },
      "source": [
        "# Joins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZw41ki6kc3p"
      },
      "source": [
        "## Ejercicio 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAzx5B5IkVeP"
      },
      "source": [
        "**Crea los dataframes correspondientes para los ficheros \"characters.csv\" y \"planets.csv\". <br/>\n",
        "Obtén la gravedad del planeta para cada personaje. Selecciona sólo el nombre del personaje y planeta además de su gravedad**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMlJTJzTkC2j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSEBVnVQlU52"
      },
      "source": [
        "## Ejercicio 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF4fyAaMlXsI"
      },
      "source": [
        "**Revisa el plan de ejecución del ejercicio 3. ¿Qué tipo de join se está ejecutando? ¿Por qué?**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbYUbaSMlXGc"
      },
      "source": [
        "**Después de revisar el plan de ejecución, ejecuta las siguientes instrucciones**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8HYt2Eylh4k"
      },
      "source": [
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", '0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwEnVoVqnj7D"
      },
      "source": [
        "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxqgghUnlWe3"
      },
      "source": [
        "**Vuelve a ejecutar la consulta del ejercicio 3 que contiene el Join**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT-AFIMvm43z"
      },
      "source": [
        "## Ejercicio 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9UBVU6ym9ue"
      },
      "source": [
        "**Crea un DataFrame a partir del fichero de \"species.csv\" y reparticiona este y el DataFrame de Characters a 100 particiones**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohStdyUGnRC0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv0AmHi926F7"
      },
      "source": [
        "## Ejercicio 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR6FPU3b29TP"
      },
      "source": [
        "**Obtén la clasificación de especies para cada personaje. Selecciona sólo el nombre del personaje y su clasificación de especie**<br>\n",
        "Usa los datframes reparticionados\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0tRj1VIonEX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG81SgpY4P3t"
      },
      "source": [
        "## Ejercicio 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56fGSsmt4adO"
      },
      "source": [
        "**Ejecuta la siguiente operación sobre el DataFrame del ejercicio 6 y observa la diferencia de reparto de rows entre las particiones**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io-BtgnHyKZ6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}